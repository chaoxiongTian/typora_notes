## 交叉熵 cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1]))
先把行累加成一维,然后再就平均成一个数.
### tf.reduce_sum:
```python
reduce_sum(
    input_tensor, #表示输入
    axis=None, #表示在那个维度进行sum操作。
    keep_dims=False, #表示是否保留原始数据的维度，False相当于执行完后原始数据就会少一个维度。
    name=None,
    reduction_indices=None #:为了跟旧版本的兼容，现在已经不使用了。
)
```
```
# 'x' is [[1, 1, 1]
#         [1, 1, 1]]
tf.reduce_sum(x) ==> 6
tf.reduce_sum(x, 0) ==> [2, 2, 2]
tf.reduce_sum(x, 1) ==> [3, 3]
tf.reduce_sum(x, 1, keep_dims=True) ==> [[3], [3]]
tf.reduce_sum(x, [0, 1]) ==> 6
```
### tf.reduce_mean()  tf.reduce_max同理
```
# 'x' is [[1., 2.]
#         [3., 4.]]
tf.reduce_mean(x) ==> 2.5
tf.reduce_mean(x, 0) ==> [2.,  3.]
tf.reduce_mean(x, 1) ==> [1.5,  3.5]
```
